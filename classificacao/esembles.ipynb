{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3207db50",
   "metadata": {},
   "source": [
    "### conceito\n",
    "\n",
    "Um conjunto de previsores é chamado de esemble. Parte do princípio que diferentes previsores formam uma decisão mais robusta que um individual. Os esembles funcionam melhor quando os previsores são o mais independente dos outros quanto possível pois, desta forma, aumenta a chance de cometerem tipos de erros diferentes, meelhorando o desempenho do esemble.\n",
    "\n",
    "**Bagging:** Cada preditor é feito em cima de uma base diferente obtida por meio de bootstrap e o resultado é o conjunto dos preditores, aumentando a variabilidade. Bagging vem de bootstrap aggregate. Normalmente, os preditores são modelos/algoritmos iguais. Quando a amostragem é realizada sem substituição, é chamada de **pasting**.\n",
    "\n",
    "**Boosting:** Cada iteração repondera os dados, dando mais peso para os exemplos (instâncias) errados. Normalmente, os preditores são modelos/algoritmos iguais. Os métodos mais populares são AdaBoost e Gradient Boosting.\n",
    "\n",
    "**Stacking:** O modelo final (blender) é alimentado com os resultados dos outros modelos. Normalmente, os preditores/algoritmos são diferentes, isto é, são usados diferentes modelos para construir o metamodelo que será responsável pelo resultado final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f600e103",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29c92da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inicializando algoritmos\n",
    "\n",
    "log_clf = LogisticRegression()\n",
    "rnd_clf = RandomForestClassifier()\n",
    "svm_clf = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1c5ee00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# criar dataset\n",
    "X, y = make_moons(n_samples = 10000, noise =.5, random_state = 0)\n",
    "\n",
    "# separação entre treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d15766ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr', LogisticRegression()),\n",
       "                             ('rf', RandomForestClassifier()), ('svc', SVC())])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rodar algoritmo\n",
    "\n",
    "voting_clf = VotingClassifier(estimators = [('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)], voting = 'hard')\n",
    "\n",
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c7260c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.812\n",
      "RandomForestClassifier 0.795\n",
      "SVC 0.833\n",
      "VotingClassifier 0.8245\n"
     ]
    }
   ],
   "source": [
    "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))\n",
    "    \n",
    "\n",
    "# neste caso, o classificador de votação não supera o SVC individualmente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b05d993",
   "metadata": {},
   "source": [
    "**Random Forest:** É um esemble de árvores de decisão que utiliza bagging como método. As árvores são construídas a partir de um bootstrap dataset e apenas um conjunto aleatório específico de variáveis é selecionado em cada etapa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e6e87e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7965"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rodar algoritmo\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators = 100, max_features = \"auto\", random_state = 0)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c52cc6",
   "metadata": {},
   "source": [
    "**Adaptive Boosting (AdaBoost):** É um esemble de previsores que utiliza boosting como método. Os previsores são construídos a partir de um bootstrap dataset, mas a cada previsor é treinado com pesos ponderados pelo previsor anterior, onde as instâncias de classificações erradas recebem maior peso para serem corrigidas no próximo previsor. Ao final, quando menos a taxa de erro do previsor, mais \"influência\" na decisão ele tem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be10a2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2393f500",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.833"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = AdaBoostClassifier(n_estimators = 100)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704fe4f1",
   "metadata": {},
   "source": [
    "**Gradient Boosting:** Assim como o AdaBoost, adiciona previsores sequencialmente a um conjunto, cada um corrigindo seu antecessor. No entanto, em vez de ajustar os pesos da instância a cada iteração, tenta ajustar o novo previsor aos erros residuais feitos pelo anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9922d698",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d32bf41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8335"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = GradientBoostingClassifier(n_estimators = 100)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
